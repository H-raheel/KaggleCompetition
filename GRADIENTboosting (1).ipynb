{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfgv57kzELiG",
    "outputId": "3278321d-0e80-4139-d727-fdc40767fb7e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier,StackingClassifier,BaggingClassifier,AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer,mean_squared_error\n",
    "#from catboost import CatBoostClassifier, Pool, cv\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bPwLILa5ELiJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_set = pd.read_csv(r'C:\\Users\\hrahe\\Downloads\\train_set.csv')\n",
    "test_set = pd.read_csv(r'C:\\Users\\hrahe\\Downloads\\test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZPFWRnsELiK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxqiUOpEELiK",
    "outputId": "e345cda2-f9b1-4d5a-c785-7ae1d056914e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns: ['X4', 'X5', 'X6', 'X8', 'X10', 'X11', 'X16', 'Y']\n",
      "Numerical Columns: ['RecordId', 'X2', 'X3', 'X7', 'X9', 'X12', 'X13', 'X14', 'X15', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "for column in train_set.columns:\n",
    "  if train_set[column].dtype == object or train_set[column].nunique() < 10:\n",
    "    categorical_cols.append(column)\n",
    "  else:\n",
    "    numerical_cols.append(column)\n",
    "\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GL5KTIwhELiL",
    "outputId": "ecbd1032-cc17-4f2f-cf1a-2adc63b3dab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values for both train and test datasets\n",
    "for column in categorical_cols:\n",
    "  if column in train_set.columns and train_set[column].isnull().any():\n",
    "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    train_set[column] = mode_imputer.fit_transform(train_set[[column]])\n",
    "  if column in test_set.columns and test_set[column].isnull().any():\n",
    "    if column in train_set.columns:\n",
    "      mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "      test_set[column] = mode_imputer.fit_transform(test_set[[column]])\n",
    "    else:\n",
    "      print(f\"Warning: Column '{column}' is missing in the training set and cannot be imputed in the test set.\")\n",
    "\n",
    "for column in numerical_cols:\n",
    "  if column in train_set.columns and train_set[column].isnull().any():\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    train_set[column] = mean_imputer.fit_transform(train_set[[column]])\n",
    "  if column in test_set.columns and test_set[column].isnull().any():\n",
    "    if column in train_set.columns:\n",
    "      mean_imputer = SimpleImputer(strategy='mean')\n",
    "      test_set[column] = mean_imputer.fit_transform(test_set[[column]])\n",
    "    else:\n",
    "      print(f\"Warning: Column '{column}' is missing in the training set and cannot be imputed in the test set.\")\n",
    "\n",
    "\n",
    "# Find missing values in the training set\n",
    "missing_values = train_set.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Find missing values in the test set\n",
    "missing_values = test_set.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhg7DglLELiL",
    "outputId": "20e725e5-5cb2-4117-fb7f-8626cdf80b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12',\n",
       "       'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22',\n",
       "       'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32',\n",
       "       'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42',\n",
       "       'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52',\n",
       "       'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62',\n",
       "       'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72',\n",
       "       'X73', 'X74', 'X75', 'X76', 'X77', 'X78'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming train_set and test_set are pandas DataFrames\n",
    "# Get all columns except 'Y' for X\n",
    "X = train_set[[col for col in train_set.columns if col != 'Y']]\n",
    "\n",
    "# Get only 'Y' column for y\n",
    "y = train_set['Y']\n",
    "\n",
    "# Select the same features for the test data\n",
    "X_testdata = test_set[[col for col in test_set.columns if col != 'Y']]\n",
    "if 'RecordId' in X.columns:\n",
    "  X = X.drop('RecordId', axis=1)\n",
    "if 'RecordId' in X_testdata.columns:\n",
    "  X_testdata = X_testdata.drop('RecordId', axis=1)\n",
    "\n",
    "# ... rest of your code (scaling, feature selection, model training, etc.) ...\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njx_b3_dELiM"
   },
   "outputs": [],
   "source": [
    "# scalar=MinMaxScaler()\n",
    "# X = scalar.fit_transform(X)\n",
    "# X_testdata = scalar.transform(X_testdata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "JpTqueoLRuwL",
    "outputId": "11efaf5a-6098-4480-c422-cd2e643d6932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features - AUC: 0.9437\n",
      "Top 15 features - AUC: 0.9500\n",
      "Top 20 features - AUC: 0.9586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f41096725be5>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Re-train the model with the top n features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Predict probabilities on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1282\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1285\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    305\u001b[0m             )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4134\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             _safe_call(\n\u001b[0;32m-> 4136\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4137\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4138\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import lightgbm as lgb\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# # Initialize and train the LightGBM model on full feature set to get feature importances\n",
    "# clf = lgb.LGBMClassifier(\n",
    "#     objective='binary',\n",
    "#     metric='binary_logloss',\n",
    "#     boosting_type='gbdt',\n",
    "#     num_leaves=20,\n",
    "#     max_depth=4,\n",
    "#     learning_rate=0.03,\n",
    "#     n_estimators=376,\n",
    "#     subsample=0.7,\n",
    "#     colsample_bytree=0.5,\n",
    "#     min_child_weight=5,\n",
    "#     scale_pos_weight=1.1,\n",
    "#     reg_lambda=0.1,\n",
    "#     verbose=-1\n",
    "# )\n",
    "\n",
    "# # Fit the model on full data to get feature importances\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Get feature importances and create a DataFrame\n",
    "# feature_importances = clf.feature_importances_\n",
    "# feature_importances_df = pd.DataFrame({\n",
    "#     'Feature': X_train.columns,\n",
    "#     'Importance': feature_importances\n",
    "# })\n",
    "\n",
    "# # Sort the DataFrame by importance\n",
    "# feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Define different numbers of top features to test\n",
    "# feature_counts = [10, 15, 20, 25, 30,35,40,45,50,55,60]\n",
    "# auc_scores = {}\n",
    "\n",
    "# # Iterate over different numbers of top features\n",
    "# for n in feature_counts:\n",
    "#     # Select the top n features\n",
    "#     top_features = feature_importances_df['Feature'].head(n).tolist()\n",
    "\n",
    "#     # Subset the training and testing data to the top n features\n",
    "#     X_train_top = X_train[top_features]\n",
    "#     X_test_top = X_test[top_features]\n",
    "\n",
    "#     # Re-train the model with the top n features\n",
    "#     clf.fit(X_train_top, y_train)\n",
    "\n",
    "#     # Predict probabilities on the test set\n",
    "#     y_pred_proba = clf.predict_proba(X_test_top)[:, 1]\n",
    "\n",
    "#     # Calculate AUC score\n",
    "#     auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#     auc_scores[n] = auc  # Store the AUC score for this number of features\n",
    "\n",
    "#     print(f\"Top {n} features - AUC: {auc:.4f}\")\n",
    "\n",
    "# # Print all AUC scores for comparison\n",
    "# print(\"\\nAUC scores for different feature counts:\")\n",
    "# for n, auc in auc_scores.items():\n",
    "#     print(f\"Top {n} features: AUC = {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wj301VsELiM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wRkbQoMDELiM"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EMpIV7RELiN"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.03, 0.1],\n",
    "#     'max_depth': [4, 6, 8],\n",
    "#     'n_estimators': [100, 200, 376,500],\n",
    "#     'gamma': [0, 0.1, 0.2],\n",
    "#     'reg_lambda': [0.01, 0.1, 1],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "#     'colsample_bylevel': [0.5, 0.7, 1.0],\n",
    "#     'scale_pos_weight': [1, 1.1, 1.2],\n",
    "#     'subsample': [0.7, 0.8, 1.0]\n",
    "# }\n",
    "\n",
    "# # Initialize the classifier\n",
    "# clf = xgb.XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='logloss',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=clf, param_distributions=param_grid, cv=3, n_iter=50, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "# # Fit the model\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters found: \", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7EmjQZTEkeW"
   },
   "outputs": [],
   "source": [
    "# # prompt: like the following geneerate good params for catboost but stire separately:import xgboost as xgb\n",
    "# # from sklearn.model_selection import GridSearchCV\n",
    "# # # Define the parameter grid\n",
    "# # param_grid = {\n",
    "# #     'learning_rate': [0.01, 0.03, 0.1],\n",
    "# #     'max_depth': [4, 6, 8],\n",
    "# #     'n_estimators': [100, 200, 376,500],\n",
    "# #     'gamma': [0, 0.1, 0.2],\n",
    "# #     'reg_lambda': [0.01, 0.1, 1],\n",
    "# #     'min_child_weight': [1, 3, 5],\n",
    "# #     'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "# #     'colsample_bylevel': [0.5, 0.7, 1.0],\n",
    "# #     'scale_pos_weight': [1, 1.1, 1.2],\n",
    "# #     'subsample': [0.7, 0.8, 1.0]\n",
    "# # }\n",
    "# # # Initialize the classifier\n",
    "# # clf = xgb.XGBCla\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# import json\n",
    "\n",
    "# # Assuming X_train, y_train are defined from your previous code\n",
    "\n",
    "# # Define the parameter grid for CatBoost\n",
    "# param_grid = {\n",
    "#     'iterations': [100, 200, 300],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'depth': [4, 6, 8],\n",
    "#     'l2_leaf_reg': [1, 3, 5],\n",
    "#     'border_count': [32, 64, 128],\n",
    "#     'random_strength': [1, 1.5, 2],\n",
    "#     'bagging_temperature': [0, 1, 2],\n",
    "# }\n",
    "\n",
    "# # Initialize the CatBoost classifier\n",
    "# clf2 = CatBoostClassifier(\n",
    "#     loss_function='Logloss',\n",
    "#     eval_metric='AUC',\n",
    "#     task_type='GPU',  # Use GPU if available\n",
    "#     random_seed=42\n",
    "# )\n",
    "\n",
    "\n",
    "# # Initialize GridSearchCV for CatBoost\n",
    "# grid_search2 = GridSearchCV(estimator=clf2, param_grid=param_grid, cv=3, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "# # Fit the model\n",
    "# grid_search2.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters found: \", grid_search2.best_params_)\n",
    "\n",
    "# # Store the best parameters in a JSON file\n",
    "# best_params2 = grid_search2.best_params_\n",
    "# with open('best_catboost_params.json', 'w') as f:\n",
    "#     json.dump(best_params2, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DASk166tDyVN"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# # Define the parameter grid for LightGBM\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.03, 0.1],\n",
    "#     'max_depth': [4, 6, 8, -1],  # -1 indicates no limit for depth in LightGBM\n",
    "#     'n_estimators': [100, 200, 376, 500],\n",
    "#     'num_leaves': [20, 31, 40, 50],  # Controls complexity in LightGBM\n",
    "#     'reg_lambda': [0.01, 0.1, 1],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "#     'subsample': [0.7, 0.8, 1.0],\n",
    "#     'scale_pos_weight': [1, 1.1, 1.2]\n",
    "# }\n",
    "\n",
    "# # Initialize the classifier\n",
    "# clf = lgb.LGBMClassifier(\n",
    "#     objective='binary',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=clf,\n",
    "#     param_distributions=param_grid,\n",
    "#     cv=3,\n",
    "#     n_iter=50,\n",
    "#     scoring='roc_auc',\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Fit the model\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters found: \", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okZsY6SIELiN"
   },
   "outputs": [],
   "source": [
    "# clf =xgb.XGBClassifier(\n",
    "#       subsample=1.0,\n",
    "#       scale_pos_weight= 1.2,\n",
    "#       reg_lambda= 0.01,\n",
    "#       n_estimators= 500,\n",
    "#       min_child_weight= 5,\n",
    "#       max_depth= 6,\n",
    "#       learning_rate= 0.03,\n",
    "#       gamma= 0.2,\n",
    "#       colsample_bytree= 0.7,\n",
    "#       colsample_bylevel=0.7,\n",
    "#       objective='binary:logistic',\n",
    "#     eval_metric='logloss',\n",
    "#     random_state=42\n",
    "\n",
    "# )\n",
    "\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "NFuNQUL9OlVv",
    "outputId": "03a1c473-29b2-4435-9ecc-561021aad109"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize the Gradient Boosting classifier with optimal parameters\n",
    "clf = GradientBoostingClassifier(\n",
    "    learning_rate=0.01,     # Set the optimal learning rate\n",
    "    n_estimators=1210,       # Set the optimal number of estimators\n",
    "    max_depth=4,            # Set the optimal max depth\n",
    "    min_samples_leaf=5,     # Set the optimal min samples per leaf\n",
    "    subsample=0.8,          # Set the optimal subsample ratio\n",
    "    max_features=0.7,       # Set the optimal max features\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJ1El570Rr6P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "meGHD5NcELiO",
    "outputId": "11f13ec7-eec7-4544-f9f7-8a453ee77d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Importance\n",
      "30     X32    0.184788\n",
      "38     X40    0.050030\n",
      "68     X70    0.040623\n",
      "7       X9    0.038002\n",
      "67     X69    0.037419\n",
      "..     ...         ...\n",
      "73     X75    0.000519\n",
      "74     X76    0.000241\n",
      "2       X4    0.000132\n",
      "69     X71    0.000000\n",
      "14     X16    0.000000\n",
      "\n",
      "[77 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(feature_importances_df)\n",
    "\n",
    "top_features = feature_importances_df['Feature'].head(30).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkulpzgFNWL7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX12JdP46fNh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier  # Or your preferred classifier\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Initialize variables\n",
    "# selected_features = []\n",
    "# remaining_features = X_train.columns.tolist()\n",
    "# top_features = []  # Store selected top features\n",
    "# max_features = 10  # Define the maximum number of features to select\n",
    "\n",
    "# # Perform forward selection using AUC-ROC as the scoring metric\n",
    "# for i in range(max_features):\n",
    "#     scores = []\n",
    "#     # Test each remaining feature by adding it to the selected_features\n",
    "#     for feature in remaining_features:\n",
    "#         # Define the current set of features\n",
    "#         current_features = selected_features + [feature]\n",
    "\n",
    "#         # Perform cross-validation using only the current set of features\n",
    "#         score = cross_val_score(clf, X_train[current_features], y_train, cv=5, scoring='roc_auc').mean()\n",
    "#         scores.append((feature, score))\n",
    "\n",
    "#     # Find the feature with the best score\n",
    "#     best_feature, best_feature_score = max(scores, key=lambda x: x[1])\n",
    "\n",
    "#     # If we still want to select features, update selected_features\n",
    "#     selected_features.append(best_feature)\n",
    "#     remaining_features.remove(best_feature)\n",
    "#     top_features.append(best_feature)\n",
    "\n",
    "# # Output the selected top features\n",
    "# print(\"Top selected features:\", top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyN4-Zg5ELiO"
   },
   "outputs": [],
   "source": [
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "X_testdata_top = X_testdata[top_features]\n",
    "\n",
    "# Re-train the model with the top features\n",
    "clf.fit(X_train_top, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w-9GdFlGELiP",
    "outputId": "0c08260d-2ae5-4e26-9fe8-85087e7b4340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.932137775746821\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test_top)[:, 1]\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "##should be new if not running.current 0.932137775746821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_top = X[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "X_testdata_top = X_testdata[top_features]\n",
    "\n",
    "# Re-train the model with the top features\n",
    "clf.fit(X_train_top, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwNnYCIrELiP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Jnwq4bxELiP"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities for the test set\n",
    "y_pred_proba_testdata = clf.predict_proba(X_testdata_top)[:, 1]\n",
    "\n",
    "# Create a DataFrame with RecordId and predicted probabilities\n",
    "test_set['Y_probability'] = y_pred_proba_testdata\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "test_set[['RecordId', 'Y_probability']].to_csv('test_set_with_probabilities.csv', index=False)\n",
    "\n",
    "print(\"Probabilities saved to test_set_with_probabilities.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the generated CSV file\n",
    "csv_file_path = 'test_set_with_probabilities.csv'\n",
    "test_set_with_probabilities = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get unique values in the Y_probability column\n",
    "unique_y_probabilities = test_set_with_probabilities['Y_probability'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_y_probabilities)\n",
    "print(test_set_with_probabilities)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
