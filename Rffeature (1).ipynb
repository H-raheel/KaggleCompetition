{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ialn0CNCC1wr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set=pd.read_csv('train_set.csv')\n",
        "test_set=pd.read_csv('test_set.csv')"
      ],
      "metadata": {
        "id": "6O5EubnxDKD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = []\n",
        "numerical_cols = []\n",
        "\n",
        "for column in train_set.columns:\n",
        "  if train_set[column].dtype == object or train_set[column].nunique() < 10:\n",
        "    categorical_cols.append(column)\n",
        "  else:\n",
        "    numerical_cols.append(column)\n",
        "\n",
        "print(\"Categorical Columns:\", categorical_cols)\n",
        "print(\"Numerical Columns:\", numerical_cols)"
      ],
      "metadata": {
        "id": "jNpXj1t8DWQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for both train and test datasets\n",
        "for column in categorical_cols:\n",
        "  if column in train_set.columns and train_set[column].isnull().any():\n",
        "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    train_set[column] = mode_imputer.fit_transform(train_set[[column]])\n",
        "  if column in test_set.columns and test_set[column].isnull().any():\n",
        "    if column in train_set.columns:\n",
        "      mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "      test_set[column] = mode_imputer.fit_transform(test_set[[column]])\n",
        "    else:\n",
        "      print(f\"Warning: Column '{column}' is missing in the training set and cannot be imputed in the test set.\")\n",
        "\n",
        "for column in numerical_cols:\n",
        "  if column in train_set.columns and train_set[column].isnull().any():\n",
        "    mean_imputer = SimpleImputer(strategy='mean')\n",
        "    train_set[column] = mean_imputer.fit_transform(train_set[[column]])\n",
        "  if column in test_set.columns and test_set[column].isnull().any():\n",
        "    if column in train_set.columns:\n",
        "      mean_imputer = SimpleImputer(strategy='mean')\n",
        "      test_set[column] = mean_imputer.fit_transform(test_set[[column]])\n",
        "    else:\n",
        "      print(f\"Warning: Column '{column}' is missing in the training set and cannot be imputed in the test set.\")\n",
        "\n",
        "\n",
        "# Find missing values in the training set\n",
        "missing_values = train_set.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Find missing values in the test set\n",
        "missing_values = test_set.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n"
      ],
      "metadata": {
        "id": "v853PxO6EI-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming train_set and test_set are pandas DataFrames\n",
        "# Get all columns except 'Y' for X\n",
        "X = train_set[[col for col in train_set.columns if col != 'Y']]\n",
        "\n",
        "# Get only 'Y' column for y\n",
        "y = train_set['Y']\n",
        "\n",
        "# Select the same features for the test data\n",
        "X_testdata = test_set[[col for col in test_set.columns if col != 'Y']]\n",
        "if 'RecordId' in X.columns:\n",
        "  X = X.drop('RecordId', axis=1)\n",
        "if 'RecordId' in X_testdata.columns:\n",
        "  X_testdata = X_testdata.drop('RecordId', axis=1)\n",
        "\n",
        "# ... rest of your code (scaling, feature selection, model training, etc.) ...\n",
        "X.columns"
      ],
      "metadata": {
        "id": "V2quDZU8EL5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "dGBovNgwFEcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Train Extra Trees Classifier on the training data\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=1000,  #  number of trees\n",
        "    max_depth=5,       # Limit tree depth to control overfitting-->change depth ab\n",
        "    min_samples_split=10,  # Minimum samples required to split\n",
        "    min_samples_leaf=2,    # Minimum samples required at a leaf node\n",
        "    max_features='sqrt',   # Randomly select a subset of features\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Option 1: Using mean threshold\n",
        "# selector_mean = SelectFromModel(clf, threshold=\"mean\", prefit=True)\n",
        "# X_selected_mean = selector_mean.transform(X)\n",
        "# X_test_selected_mean = selector_mean.transform(X_testdata)\n",
        "\n",
        "# Option 2: Using a custom threshold value\n",
        "custom_threshold = 0.5  # Set a custom threshold for feature importance\n",
        "selector_custom = SelectFromModel(clf, threshold=custom_threshold, prefit=True)\n",
        "X_selected_custom = selector_custom.transform(X)\n",
        "X_test_selected_custom = selector_custom.transform(X_testdata)\n",
        "\n",
        "# Option 3: Using a quantile threshold (e.g., top 25% features)\n",
        "# import numpy as np\n",
        "\n",
        "# feature_importances = clf.feature_importances_\n",
        "# quantile_threshold = np.percentile(feature_importances, 90)  # Top 10% most important features\n",
        "# selector_quantile = SelectFromModel(clf, threshold=quantile_threshold, prefit=True)\n",
        "# X_selected_quantile = selector_quantile.transform(X)\n",
        "# X_test_selected_quantile = selector_quantile.transform(X_testdata)\n",
        "\n",
        "# Get the selected feature names for each threshold\n",
        "# selected_features_mean = X.columns[selector_mean.get_support()]\n",
        "# selected_features_custom = X.columns[selector_custom.get_support()]\n",
        "# selected_features_quantile = X.columns[selector_quantile.get_support()]\n",
        "\n",
        "# print(\"Selected features with mean threshold:\", selected_features_mean)\n",
        "print(\"Selected features with custom threshold:\", selected_features_custom)\n",
        "# print(\"Selected features with quantile threshold:\", selected_features_quantile)\n"
      ],
      "metadata": {
        "id": "qib6jcw3EOp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming feature names are known\n",
        "feature_names = [f'X{i}' for i in range(X.shape[1])]  # Replace with actual feature names\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# # Then use X_df in the code\n",
        "# selected_features_mean = X_df.columns[selector_mean.get_support()]\n",
        "selected_features_custom = X_df.columns[selector_custom.get_support()]\n",
        "# selected_features_quantile = X_df.columns[selector_quantile.get_support()]\n",
        "\n",
        "# print(\"Selected features with mean threshold:\", selected_features_mean)\n",
        "print(\"Selected features with custom threshold:\", selected_features_custom)\n",
        "# print(\"Selected features with quantile threshold:\", selected_features_quantile)\n"
      ],
      "metadata": {
        "id": "jvUNnbpgcH2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=1500,  #  number of trees\n",
        "    max_depth=5,       # Limit tree depth to control overfitting-->change depth ab\n",
        "    min_samples_split=10,  # Minimum samples required to split\n",
        "    min_samples_leaf=2,    # Minimum samples required at a leaf node\n",
        "    max_features='sqrt',   # Randomly select a subset of features\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_4RXGI6sEyLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate AUC\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(f'AUC: {auc_score}')"
      ],
      "metadata": {
        "id": "c_L8dcjTFLQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X, y)"
      ],
      "metadata": {
        "id": "K3-6m6wVFQJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict probabilities for the test set\n",
        "y_test_probs = clf.predict_proba(X_testdata)[:, 1]\n",
        "\n",
        "# Create a DataFrame with RecordId and predicted probabilities\n",
        "test_set['Y_probability'] = y_test_probs\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "test_set[['RecordId', 'Y_probability']].to_csv('test_set_with_probabilities.csv', index=False)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the generated CSV file\n",
        "csv_file_path = 'test_set_with_probabilities.csv'\n",
        "test_set_with_probabilities = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Get unique values in the Y_probability column\n",
        "unique_y_probabilities = test_set_with_probabilities['Y_probability'].unique()\n",
        "\n",
        "# Print the unique values\n",
        "print(unique_y_probabilities)\n",
        "print(test_set_with_probabilities)\n"
      ],
      "metadata": {
        "id": "E-kIN3YRFST-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}