{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87171,"databundleVersionId":9897270,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier,StackingClassifier,BaggingClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.metrics import  roc_auc_score\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.tree import DecisionTreeClassifier\nimport os\n# Import PCA from sklearn.decomposition\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T19:59:41.074036Z","iopub.execute_input":"2024-10-25T19:59:41.074467Z","iopub.status.idle":"2024-10-25T19:59:41.153378Z","shell.execute_reply.started":"2024-10-25T19:59:41.074428Z","shell.execute_reply":"2024-10-25T19:59:41.152067Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_set = pd.read_csv(\"/kaggle/input/dataset/train_set.csv\")\ntest_set = pd.read_csv(\"/kaggle/input/dataset/test_set.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:00:28.645265Z","iopub.execute_input":"2024-10-25T20:00:28.645761Z","iopub.status.idle":"2024-10-25T20:00:28.930839Z","shell.execute_reply.started":"2024-10-25T20:00:28.645684Z","shell.execute_reply":"2024-10-25T20:00:28.929090Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/dataset/train_set.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_set \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset/test_set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/dataset/train_set.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/dataset/train_set.csv'","output_type":"error"}]},{"cell_type":"code","source":"\n# Find missing values in the training set\nmissing_values = train_set.isnull().sum()\nprint(missing_values[missing_values > 0])\n\n# Find missing values in the test set\nmissing_values = test_set.isnull().sum()\nprint(missing_values[missing_values > 0])","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.793846Z","iopub.status.idle":"2024-10-25T19:58:14.794300Z","shell.execute_reply.started":"2024-10-25T19:58:14.794087Z","shell.execute_reply":"2024-10-25T19:58:14.794108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols = []\nnumerical_cols = []\n\nfor column in train_set.columns:\n  if train_set[column].dtype == object or train_set[column].nunique() < 10:\n    categorical_cols.append(column)\n  else:\n    numerical_cols.append(column)\n\nprint(\"Categorical Columns:\", categorical_cols)\nprint(\"Numerical Columns:\", numerical_cols)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.795777Z","iopub.status.idle":"2024-10-25T19:58:14.796231Z","shell.execute_reply.started":"2024-10-25T19:58:14.796018Z","shell.execute_reply":"2024-10-25T19:58:14.796040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle missing values for both train and test datasets\nfor column in categorical_cols:\n  if column in train_set.columns and train_set[column].isnull().any():\n    mode_imputer = SimpleImputer(strategy='most_frequent')\n    train_set[column] = mode_imputer.fit_transform(train_set[[column]])\n  if column in test_set.columns and test_set[column].isnull().any():\n    if column in train_set.columns:\n      mode_imputer = SimpleImputer(strategy='most_frequent')\n      test_set[column] = mode_imputer.fit_transform(test_set[[column]])\n    else:\n      print(f\"Warning: Column '{column}' is missing in the training set and cannot be imputed in the test set.\")\n\nfor column in numerical_cols:\n  if column in train_set.columns and train_set[column].isnull().any():\n    mean_imputer = SimpleImputer(strategy='mean')\n    train_set[column] = mean_imputer.fit_transform(train_set[[column]])\n  if column in test_set.columns and test_set[column].isnull().any():\n    if column in train_set.columns:\n      mean_imputer = SimpleImputer(strategy='mean')\n      test_set[column] = mean_imputer.fit_transform(test_set[[column]])\n    else:\n      print(f\"Warning: Column '{column}' is missing in the training set and cannot be imputed in the test set.\")\n    \n    \n# Find missing values in the training set\nmissing_values = train_set.isnull().sum()\nprint(missing_values[missing_values > 0])\n\n# Find missing values in the test set\nmissing_values = test_set.isnull().sum()\nprint(missing_values[missing_values > 0])    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.798429Z","iopub.status.idle":"2024-10-25T19:58:14.798923Z","shell.execute_reply.started":"2024-10-25T19:58:14.798653Z","shell.execute_reply":"2024-10-25T19:58:14.798675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Assuming train_set and test_set are pandas DataFrames\n# Get all columns except 'Y' for X\nX = train_set[[col for col in train_set.columns if col != 'Y']]\n\n# Get only 'Y' column for y\ny = train_set['Y']\n\n# Select the same features for the test data\nX_testdata = test_set[[col for col in test_set.columns if col != 'Y']]\nif 'RecordId' in X.columns:\n  X = X.drop('RecordId', axis=1)\nif 'RecordId' in X_testdata.columns:\n  X_testdata = X_testdata.drop('RecordId', axis=1)\n\n# ... rest of your code (scaling, feature selection, model training, etc.) ...\nX.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.800947Z","iopub.status.idle":"2024-10-25T19:58:14.801519Z","shell.execute_reply.started":"2024-10-25T19:58:14.801226Z","shell.execute_reply":"2024-10-25T19:58:14.801255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalar=MinMaxScaler()\nX = scalar.fit_transform(X)\nX_testdata = scalar.transform(X_testdata)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.803200Z","iopub.status.idle":"2024-10-25T19:58:14.803623Z","shell.execute_reply.started":"2024-10-25T19:58:14.803411Z","shell.execute_reply":"2024-10-25T19:58:14.803432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.805433Z","iopub.status.idle":"2024-10-25T19:58:14.805915Z","shell.execute_reply.started":"2024-10-25T19:58:14.805656Z","shell.execute_reply":"2024-10-25T19:58:14.805678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the ExtraTreesClassifier with appropriate hyperparameters\nclf = ExtraTreesClassifier(\n    n_estimators=100,  # Number of trees in the forest\n    max_depth=10,      # Maximum depth of each tree (adjust as needed)\n    min_samples_split=9,  # Minimum number of samples required to split a node\n    min_samples_leaf=50,   # Minimum number of samples required at each leaf node\n    #max_features='sqrt',  # Number of features to consider when looking for the best split\n    random_state=42\n)\nclf.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.807389Z","iopub.status.idle":"2024-10-25T19:58:14.807866Z","shell.execute_reply.started":"2024-10-25T19:58:14.807609Z","shell.execute_reply":"2024-10-25T19:58:14.807630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\n\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\n\n\n\n# Calculate AUC\nauc_score = roc_auc_score(y_test, y_probs)\nprint(f'AUC: {auc_score}')","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.809261Z","iopub.status.idle":"2024-10-25T19:58:14.809667Z","shell.execute_reply.started":"2024-10-25T19:58:14.809462Z","shell.execute_reply":"2024-10-25T19:58:14.809482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Predict probabilities for the test set\ny_test_probs = clf.predict_proba(X_testdata)[:, 1]\n\n# Create a DataFrame with RecordId and predicted probabilities\ntest_set['Y_probability'] = y_test_probs\n\n# Save the DataFrame to a CSV file\ntest_set[['RecordId', 'Y_probability']].to_csv('test_set_with_probabilities.csv', index=False)\n\n\nimport pandas as pd\n\n# Read the generated CSV file\ncsv_file_path = 'test_set_with_probabilities.csv'\ntest_set_with_probabilities = pd.read_csv(csv_file_path)\n\n# Get unique values in the Y_probability column\nunique_y_probabilities = test_set_with_probabilities['Y_probability'].unique()\n\n# Print the unique values\nprint(unique_y_probabilities)\nprint(test_set_with_probabilities)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:58:14.810893Z","iopub.status.idle":"2024-10-25T19:58:14.811347Z","shell.execute_reply.started":"2024-10-25T19:58:14.811123Z","shell.execute_reply":"2024-10-25T19:58:14.811145Z"},"trusted":true},"execution_count":null,"outputs":[]}]}